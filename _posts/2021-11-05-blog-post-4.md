---
layout: post
title: Blog Post 4
---

In this post, we're going to implement the spectral clustering algorithm for clustering data points using the `numpy` and `scipy` libraries. To begin, we'll optimize an objective function to output the data classes, but later we'll use something called a Laplacian matrix which simplifies the clustering process. 

## Introduction

Clustering methods like K-means are used for simple clustering problems—however, when data is shaped "weirdly", (e.g. two crescent moons next to each other or two cocentric circles), K-means often fails to classify the data (e.g. each of the crescent moons or each of the cocentric circles). Below is an example of the crescent moon data.

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![plot-4-1.png]({{ site.baseurl }}/assets/images/plot-4-1.png)

Spectral clustering is a method that aims to cluster data with complex structure. Below we use optimization to implement the method, before using a shortcut using Laplacian matrices. 

## Similarity Matrix

We create a similarity matrix `A`, which is a `n x n` matrix that uses the Euclidean distance between points `X[i]` and `X[j]`. Here, if the distance is less than `epsilon` (a parameter we can change), entry `A[i, j]` is `1`, but if the distance is more than `epsilon`, entry `A[i, j]` is `0`. 

While the Euclidean distance between the same point is `0` which implies `A[i, j]` is `1` if `epsilon > 0`, we make all diagonal entries `A[i, i] 0`. We create the similarity matrix below.

```python
from sklearn.metrics import pairwise_distances

# set epsilon = 0.4
epsilon = 0.4

# pairwise_distances() with conditional indexing outputs the similarity matrix with 1 for each diagonal entry
A = (pairwise_distances(X) < epsilon).astype(int)

# fill diagonals with 0
np.fill_diagonal(A, 0)
```

## Binary Norm Cut Objective

The next step is to create the binary norm cut objective, which contains the cut and the volume terms. For this, we also use the row-sum, a vector where each element is the sum of the rows of the similarity matrix `A`. We also use `y`, the true cluster membership vector, where each element of `y` (either `0` or `1`) indicates what class each point in `X` lies. The smaller the binary norm cut objective, the better the true cluster membership vector is at differentiating between classes.

### The Cut Term

The cut term adds up entries `A[i, j]`, where `i` indicates each index of the points in the first class and `j` indicates each index of the points in the second class. We implement a function below that calculates the cut of a matrix given the true cluster membership vector.

```python
def cut(A, y):
    # initialize cut
    cut = 0

    # for each index of y where y[i] == 0
    for i in np.where(y == 0)[0]:

        # for each index of y where y[j] == 1
        for j in np.where(y == 1)[0]:

            # add A[i, j] to cut
            cut = cut + A[i, j]
    return cut

cut(A, y)
```

13

To show that the cut objective works, we use a vector of random integers to replace `y`, and compare the cut objective.

```python
np.random.seed(1234)
cut(A, np.random.randint(2, size = n))
```

1154

The cut objective for a vector of random integers is much larger, meaning a random vector is not good at differentiating classes.

### The Volume Term

The volume term for a cluster is the sum of row sums `d[i]`, where `i` is the index of terms in one cluster in `y`. We implement a function below that calculates the volume of a matrix given the true cluster membership vector.

```python
def vols(A, y):
    # row sum
    d = np.sum(A, axis = 1)

    # returns a tuple of the sum of row sums for the first cluster and the second cluster
    return (np.sum(d[np.where(y == 0)[0]]), np.sum(d[np.where(y == 1)[0]]))

vols(A, y)
```
(2299, 2217)

The volume of cluster 0 is 2299 and the volume of cluster 1 is 2217.

### The Normcut Objective

We implement a function calculating the normcut objective using the `cut()` and `vols()` functions above.

```python
def normcut(A, y):
    return cut(A, y) * ((1 / vols(A, y)[0]) + (1 / vols(A, y)[1]))

normcut(A, y)
```

0.011518412331615225

The normcut objective is relatively small compared to a random true cluster membership vector:

```python
np.random.seed(1234)
normcut(A, np.random.randint(2, size = n))
```

1.023364647480206

This indicates we implemented the `normcut` function correctly.

## Transformation Vector

We could minimize `normcut()` with `y`, but this is a relatively impossible task. One trick is to use a new vector `z` that contains the cluster information and then minimizing a new function.

We implement the creation of the `z` vector below:

```python
def transform(A, y):
    # positive when data belongs to zero cluster, negative when data belongs to one cluster
    z = np.where(y == 0, 1 / vols(A, y)[0], -(1 / vols(A, y)[1]))

    return z
```

With the `z` vector, we can create a new expression that equals the normcut function. We implement the creation of the expression below.

```python
z = transform(A, y)

# n x n matrix of zeros
D = np.zeros((n, n), int)

# fill the diagonals of D with the row-sums of A
np.fill_diagonal(D, np.sum(A, axis = 1))

# check if the normcut function is equal to our new expression
np.isclose(normcut(A, y), (z.T@(D - A)@z) / (z.T@D@z))
```

True

We can also show that the transpose of z multiplied by D multiplied by a vector of ones is `0`.

```python
np.isclose(z.T@D@np.ones(n), 0)
```

True

## Optimization

We minimize the expression with the constraint that the transpose of z multiplied by D multiplied by a vector of ones is `0`. We implement an `orth_obj()` function below and mimimize it, starting with a random vector.

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)

from scipy.optimize import minimize

np.random.seed(1234)

# minimize orth_obj() function, starting with a random vector of 0s and 1s
z_min = minimize(orth_obj, np.random.randint(2, size = n))
```

## Plotting Make Moons Data with Spectral Clustering

We now try out our spectral clustering algorithm, where the sign of each index of z_min corresponds to the class each index of our data belongs to.

```python
plt.scatter(X[:,0], X[:,1], c = np.where((z_min.x < 0), 0, 1))
```

![plot-4-2.png]({{ site.baseurl }}/assets/images/plot-4-2.png)

The spectral clustering works! There is one point that is misclassified but everything else is correctly classified.

## Using Eigenvalues and Eigenvectors

The above method is relatively slow, and there is a faster shortcut using eigenvalues and eigenvectors. We find the eigenvalues of the inverse of D times (D minus A), and discover that the second smallest eigenvalue's eigenvector contains the classes of the data.

```python
# constructing matrix
L = np.linalg.inv(D)@(D - A)

# finding eigenvalues and eigenvectors
Lam, U = np.linalg.eig(L)

# sorting eigenvalues from smallest to largest
ix = Lam.argsort()

# removing smallest eigenvalue and its eigenvector
Lam, U = Lam[ix], U[:,ix]

# second smallest eignvector
z_eig = U[:,1]

plt.scatter(X[:,0], X[:,1], c = np.where((z_eig < 0), 0, 1))
```

![plot-4-3.png]({{ site.baseurl }}/assets/images/plot-4-3.png)

We find that our spectral clustering algorithm works (and is a lot faster)! Now we synthesize our findings into one function, `spectral_clustering`, which takes in data and an `epsilon` value and outputs a true member cluster vector.

```python
def spectral_clustering(X, epsilon):
    """
    takes data and an epsilon value and outputs a true member cluster vector to correctly classify X into two clusters
    """

    # pairwise distance matrix based on epsilon
    A = (pairwise_distances(X) < epsilon).astype(int)

    # fill diagonals with 0
    np.fill_diagonal(A, 0)
    
    # make an n x n matrix of zeros
    D = np.zeros((n, n), int)

    # fill diagonals with row-sum values
    np.fill_diagonal(D, np.sum(A, axis = 1))

    # create Laplacian matrix
    L = np.linalg.inv(D)@(D - A)
    
    # find eigenvalues and eigenvectors
    Lam, U = np.linalg.eig(L)

    # sort eigenvalues
    ix = Lam.argsort()

    # removing smallest eigenvalue and its eigenvector
    Lam, U = Lam[ix], U[:,ix]

    # second smallest eigenvector
    z_eig = U[:,1]
    
    # return true cluster membership vector based on signs of z_eig
    return np.where((z_eig < 0), 0, 1)

plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

![plot-4-4.png]({{ site.baseurl }}/assets/images/plot-4-4.png)

Looks like the function works! There is one point on the top of the bottom moon that is incorrectly classified, however.

## Experiments

We run three experiments using a different `make_moon` dataset where the `noise` parameter is increased and the number of points is increased to `1000`.

```python
np.random.seed(1234)

# increasing the number of points to 1000
n = 1000

# make_moons where noise is increased to 0.1
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

![plot-4-5.png]({{ site.baseurl }}/assets/images/plot-4-5.png)

The clustering works relatively well; again, some top points from the bottom moon are misclassified.

```python
np.random.seed(1234)
n = 1000

# make_moons where noise is increased to 0.15
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

![plot-4-6.png]({{ site.baseurl }}/assets/images/plot-4-6.png)

Here the clustering begins to fail a little, with more top points from the bottom moon and some bottom points from the top moon misclassified.

```python
np.random.seed(1234)
n = 1000

# make_moons where noise is increased to 0.2
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.20, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

![plot-4-7.png]({{ site.baseurl }}/assets/images/plot-4-7.png)

Looks like our spectral clustering algorithm doesn't work as well—some of the points near the right end of the top moon and near the left end of the bottom moon are mixed. To be fair, however, a person would have difficulty separating the moons as well.

## Bull's Eye Data Set

We test our spectral clustering algorithm on the bull's eye data set:

```python
np.random.seed(1234)

# 1000 data points
n = 1000

# make two cocentric circles
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![plot-4-8.png]({{ site.baseurl }}/assets/images/plot-4-8.png)

```python
np.random.seed(1234)

# plot with spectral_clustering 
epsilon = 0.4
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

![plot-4-9.png]({{ site.baseurl }}/assets/images/plot-4-9.png)

Looks like our spectral clustering algorithm works! Anytime we have a data set with "weirdly" shaped data, we can easily classify it.