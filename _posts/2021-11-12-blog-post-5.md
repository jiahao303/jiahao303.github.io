---
title: Blog Post 5
---

In this post, we're going to teach a convolutional neural network to distinguish between pictures of cats and dogs using the `tensorflow` library. We'll use `Datasets` in `tensorflow`, and use data augmentation and transfer learning to improve our original model.

## Load Packages and Obtain Data

To begin, we load packages and obtain our dataset. We load the following packages:

```python
import os
from tensorflow.keras import utils, layers 
import tensorflow as tf
import matplotlib.pyplot as plt
```

To obtain our dataset, we use Google's repository of dog and cat photos. We run the following code block to do so.

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

From here, we've created three `Datasets`: `train_dataset`, `validation_dataset`, and `test_dataset`. We also note that each image is `160 x 160` pixels.

We also run technical code to rapidly read our data.

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

## Working with Datasets

To visualize our dataset, we write a function that produces two rows of images: three images of cats in the first row and three images of dogs in the second row. 

```python
def two_row_visualization(train_dataset):

  # size of plot
  plt.figure(figsize=(10, 10))

  # for each image and label in one batch of 32 images
  for images, label in train_dataset.take(1):
    i = 0
    j = 0
    cats = 0
    dogs = 0

    # while the number of cat photos is less than three
    while cats < 3:

      # if ith image is a cat
      if label[i].numpy() == 0:

        # plot image as the cats + 1th image on the 2 x 3 grid
        ax = plt.subplot(2, 3, cats + 1)

        plt.imshow(images[i].numpy().astype("uint8"))

        # add cats label
        plt.title("cats")
        plt.axis("off")

        # plotted additional cat
        cats = cats + 1
      # move to next image
      i = i + 1

    # while the number of dog photos is less than three
    while dogs < 3:

      # if jth image is a dog
      if label[j].numpy() == 1:
        
        # plot image as the dogs + 4th image on the 2 x 3 grid
        ax = plt.subplot(2, 3, dogs + 4)
        
        plt.imshow(images[j].numpy().astype("uint8"))

        # add dogs label
        plt.title("dogs")

        plt.axis("off")

        # plotted additional dog
        dogs = dogs + 1
      # move to next image
      j = j + 1
    
two_row_visualization(train_dataset)
```

![plot-5-1.png]({{ site.baseurl }}/assets/images/plot-5-1.png)

## Check Label Frequencies

We create an iterator to help us see the number of images that are cats and the number of images that are dogs in the dataset. 

```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()

count = 0

# for each image in train_dataset
for i in range(2000):

  # adds 1 if image is dog and 0 if image is cat
  count = count + labels_iterator.next()
print(count)
```

1000

Since our loop adds `0` if the image is of a cat and adds `1` if the image is of a dog, we see that there are `1000` images of cats in our dataset, which implies that there are `1000` images of dogs in our dataset. 

The baseline machine learning model is the model that always guesses the most frequent label. Since our dataset is evenly split between cat and dog images, our baseline model would be 50% accurate. Any model that is better than 50% accuracy would be an improvement!

## First Model

We use `tf.keras.Sequential` and a few `tf.keras.layers` to create our convolutional neural network. Below is our first model.

```python
model1 = tf.keras.Sequential([

    # alteration of Conv2D and MaxPooling2D layers, with increase in number of filters for Conv2D layer and input_shape of 160 x 160 pixels and 3 values for RGB
    layers.Conv2D(32, (3, 3), activation = "relu", input_shape = (160, 160, 3)),

    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation = "relu"),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation = "relu"),
    layers.Flatten(),
    layers.Dropout(0.2),

    # final Dense layer of two classes (cats and dogs)
    layers.Dense(2)
])

# use adam optimizier and SparseCategoricalCrossentropy loss function, with accuracy as a metric
model1.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ["accuracy"])

# train model on 20 epochs
history = model1.fit(train_dataset, 
                     epochs = 20, 
                     validation_data = validation_dataset)

# plot training accuracy over epochs
plt.plot(history.history["accuracy"], label = "training")

# plot validation accuracy over epochs
plt.plot(history.history["val_accuracy"], label = "validation")

# set labels
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")

# show legend of training and validation
plt.legend()
```

Epoch 1/20
63/63 [==============================] - 6s 83ms/step - loss: 20.8049 - accuracy: 0.5235 - val_loss: 0.6945 - val_accuracy: 0.5495

Epoch 2/20
63/63 [==============================] - 5s 79ms/step - loss: 0.6766 - accuracy: 0.5635 - val_loss: 0.6907 - val_accuracy: 0.5495

Epoch 3/20
63/63 [==============================] - 5s 80ms/step - loss: 0.6327 - accuracy: 0.6510 - val_loss: 0.6949 - val_accuracy: 0.5804

Epoch 4/20
63/63 [==============================] - 5s 80ms/step - loss: 0.5654 - accuracy: 0.6975 - val_loss: 0.7346 - val_accuracy: 0.5631

Epoch 5/20
63/63 [==============================] - 5s 79ms/step - loss: 0.4805 - accuracy: 0.7510 - val_loss: 0.8318 - val_accuracy: 0.5644

Epoch 6/20
63/63 [==============================] - 5s 79ms/step - loss: 0.3830 - accuracy: 0.8290 - val_loss: 0.8712 - val_accuracy: 0.5767

Epoch 7/20
63/63 [==============================] - 5s 80ms/step - loss: 0.3214 - accuracy: 0.8620 - val_loss: 1.0033 - val_accuracy: 0.5755

Epoch 8/20
63/63 [==============================] - 5s 79ms/step - loss: 0.2507 - accuracy: 0.8915 - val_loss: 1.1212 - val_accuracy: 0.5545

Epoch 9/20
63/63 [==============================] - 5s 78ms/step - loss: 0.2071 - accuracy: 0.9185 - val_loss: 1.1426 - val_accuracy: 0.5854

Epoch 10/20
63/63 [==============================] - 5s 82ms/step - loss: 0.1860 - accuracy: 0.9300 - val_loss: 1.2797 - val_accuracy: 0.5842

Epoch 11/20
63/63 [==============================] - 5s 81ms/step - loss: 0.3016 - accuracy: 0.8775 - val_loss: 1.3207 - val_accuracy: 0.5347

Epoch 12/20
63/63 [==============================] - 5s 83ms/step - loss: 0.2156 - accuracy: 0.9125 - val_loss: 1.5196 - val_accuracy: 0.5347

Epoch 13/20
63/63 [==============================] - 5s 80ms/step - loss: 0.1923 - accuracy: 0.9275 - val_loss: 1.6702 - val_accuracy: 0.5470

Epoch 14/20
63/63 [==============================] - 5s 79ms/step - loss: 0.1518 - accuracy: 0.9480 - val_loss: 1.7625 - val_accuracy: 0.5866

Epoch 15/20
63/63 [==============================] - 5s 80ms/step - loss: 0.0964 - accuracy: 0.9720 - val_loss: 1.9471 - val_accuracy: 0.6002

Epoch 16/20
63/63 [==============================] - 5s 81ms/step - loss: 0.1254 - accuracy: 0.9650 - val_loss: 1.9199 - val_accuracy: 0.5965

Epoch 17/20
63/63 [==============================] - 5s 80ms/step - loss: 0.0875 - accuracy: 0.9730 - val_loss: 1.8601 - val_accuracy: 0.5928

Epoch 18/20
63/63 [==============================] - 5s 80ms/step - loss: 0.0694 - accuracy: 0.9775 - val_loss: 2.1983 - val_accuracy: 0.5928

Epoch 19/20
63/63 [==============================] - 5s 79ms/step - loss: 0.0819 - accuracy: 0.9770 - val_loss: 2.2019 - val_accuracy: 0.5854

Epoch 20/20
63/63 [==============================] - 5s 79ms/step - loss: 0.0844 - accuracy: 0.9745 - val_loss: 2.2278 - val_accuracy: 0.5804

<matplotlib.legend.Legend at 0x7f9f2fa8c2d0>

![plot-5-2.png]({{ site.baseurl }}/assets/images/plot-5-2.png)

**The accuracy of my model stabilized between 55% and 60% during training.**

I did 5-10% better than the baseline.

In the visualization, training accuracy exceeds validation accuracy in later epochs. This indicates overfitting occurred.

## Model with Data Augmentation

In our second model, we add some data augmentation layers to our model. Specifically, we introduce random flipping of images and random rotations of images to our model to help our model learn invariant features of our input images.

We demonstrate `tf.keras.layers.RandomFlip()` first.

```python
data_augmentation = tf.keras.Sequential([
  # no parameters imply both horiziontal and vertical flipping
  tf.keras.layers.RandomFlip(),
])

for image, _ in train_dataset.take(1):

  # size of plot
  plt.figure(figsize = (10, 10))

  # first image is first in image array
  first_image = image[0]

  # show nine images
  for i in range(9):

    # plot image as the ith + 1 image on the 3 x 3 grid
    ax = plt.subplot(3, 3, i + 1)

    # apply random flip
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```

![plot-5-3.png]({{ site.baseurl }}/assets/images/plot-5-3.png)

We then demonstrate `tf.keras.layers.RandomRotation()`.

```python

# parameter of 0.2 implies image will be rotated between 0.2 * 2pi radians clockwise and 0.2 * 2pi radians counterclockwise
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomRotation(0.2),
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize = (10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```

![plot-5-4.png]({{ site.baseurl }}/assets/images/plot-5-4.png)

We then create our second model, this time with data augmentation.

```python

# addition of RandomFlip() and RandomRotation(0.2)
model2 = tf.keras.Sequential([
    layers.RandomFlip(),
    layers.RandomRotation(0.2),
    layers.Conv2D(32, (3, 3), activation = "relu", input_shape = (160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation = "relu"),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation = "relu"),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(2)
])

model2.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ["accuracy"])

history = model2.fit(train_dataset, 
                     epochs = 20, 
                     validation_data = validation_dataset)

plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

Epoch 1/20
63/63 [==============================] - 7s 85ms/step - loss: 14.0817 - accuracy: 0.4990 - val_loss: 0.7193 - val_accuracy: 0.5161

Epoch 2/20
63/63 [==============================] - 6s 83ms/step - loss: 0.7027 - accuracy: 0.5250 - val_loss: 0.7058 - val_accuracy: 0.5718

Epoch 3/20
63/63 [==============================] - 6s 85ms/step - loss: 0.6966 - accuracy: 0.5395 - val_loss: 0.6996 - val_accuracy: 0.5285

Epoch 4/20
63/63 [==============================] - 5s 81ms/step - loss: 0.6886 - accuracy: 0.5420 - val_loss: 0.7036 - val_accuracy: 0.5619

Epoch 5/20
63/63 [==============================] - 5s 81ms/step - loss: 0.6879 - accuracy: 0.5690 - val_loss: 0.6946 - val_accuracy: 0.5582

Epoch 6/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6907 - accuracy: 0.5555 - val_loss: 0.7026 - val_accuracy: 0.5520

Epoch 7/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6838 - accuracy: 0.5620 - val_loss: 0.7085 - val_accuracy: 0.5260

Epoch 8/20
63/63 [==============================] - 6s 84ms/step - loss: 0.6897 - accuracy: 0.5385 - val_loss: 0.7041 - val_accuracy: 0.5532

Epoch 9/20
63/63 [==============================] - 5s 81ms/step - loss: 0.6851 - accuracy: 0.5520 - val_loss: 0.7014 - val_accuracy: 0.5408

Epoch 10/20
63/63 [==============================] - 5s 81ms/step - loss: 0.6839 - accuracy: 0.5525 - val_loss: 0.6955 - val_accuracy: 0.5718

Epoch 11/20
63/63 [==============================] - 6s 83ms/step - loss: 0.6864 - accuracy: 0.5700 - val_loss: 0.7023 - val_accuracy: 0.5705

Epoch 12/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6752 - accuracy: 0.5750 - val_loss: 0.7081 - val_accuracy: 0.5483

Epoch 13/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6863 - accuracy: 0.5375 - val_loss: 0.6978 - val_accuracy: 0.5780

Epoch 14/20
63/63 [==============================] - 6s 83ms/step - loss: 0.6717 - accuracy: 0.5885 - val_loss: 0.6914 - val_accuracy: 0.5928

Epoch 15/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6793 - accuracy: 0.5825 - val_loss: 0.7032 - val_accuracy: 0.5470

Epoch 16/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6747 - accuracy: 0.5740 - val_loss: 0.6944 - val_accuracy: 0.5520

Epoch 17/20
63/63 [==============================] - 6s 83ms/step - loss: 0.6644 - accuracy: 0.5895 - val_loss: 0.6632 - val_accuracy: 0.6139

Epoch 18/20
63/63 [==============================] - 6s 83ms/step - loss: 0.6534 - accuracy: 0.6120 - val_loss: 0.6668 - val_accuracy: 0.6040

Epoch 19/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6488 - accuracy: 0.6170 - val_loss: 0.6830 - val_accuracy: 0.5656

Epoch 20/20
63/63 [==============================] - 6s 84ms/step - loss: 0.6658 - accuracy: 0.5925 - val_loss: 0.6632 - val_accuracy: 0.6002

<matplotlib.legend.Legend at 0x7f9fb09c3bd0>

![plot-5-5.png]({{ site.baseurl }}/assets/images/plot-5-5.png)

**The accuracy of my model stabilized between 55% and 60% during training.**

My validation accuracy for `model2` is about the same as `model1`.

In the visualization, training accuracy and validation accuracy intertwine across epochs. This indicates no overfitting occurred.

## Data Preprocessing

We then consider a simple transformation to the input data—in this case, a normalization of the RGB values of our input images. We create a preprocessing layer called `preprocessor` and add it into our model.

```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])

# addition of preprocessor layer
model3 = tf.keras.Sequential([
    preprocessor,
    layers.RandomFlip(),
    layers.RandomRotation(0.2),
    layers.Conv2D(32, (3, 3), activation = "relu", input_shape = (160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation = "relu"),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation = "relu"),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(2)
])

model3.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ["accuracy"])

history = model3.fit(train_dataset, 
                     epochs = 20, 
                     validation_data = validation_dataset)

plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

Epoch 1/20
63/63 [==============================] - 7s 89ms/step - loss: 0.7268 - accuracy: 0.5365 - val_loss: 0.6582 - val_accuracy: 0.5668

Epoch 2/20
63/63 [==============================] - 6s 83ms/step - loss: 0.6561 - accuracy: 0.6015 - val_loss: 0.6304 - val_accuracy: 0.6337

Epoch 3/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6406 - accuracy: 0.6225 - val_loss: 0.7466 - val_accuracy: 0.5557

Epoch 4/20
63/63 [==============================] - 6s 84ms/step - loss: 0.6480 - accuracy: 0.5860 - val_loss: 0.6289 - val_accuracy: 0.6423

Epoch 5/20
63/63 [==============================] - 6s 82ms/step - loss: 0.6229 - accuracy: 0.6415 - val_loss: 0.6219 - val_accuracy: 0.6634

Epoch 6/20
63/63 [==============================] - 5s 82ms/step - loss: 0.6059 - accuracy: 0.6625 - val_loss: 0.6227 - val_accuracy: 0.6584

Epoch 7/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5957 - accuracy: 0.6670 - val_loss: 0.6155 - val_accuracy: 0.6559

Epoch 8/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5790 - accuracy: 0.6975 - val_loss: 0.5950 - val_accuracy: 0.6720

Epoch 9/20
63/63 [==============================] - 5s 81ms/step - loss: 0.5720 - accuracy: 0.7020 - val_loss: 0.6006 - val_accuracy: 0.6757

Epoch 10/20
63/63 [==============================] - 5s 83ms/step - loss: 0.5605 - accuracy: 0.7040 - val_loss: 0.5844 - val_accuracy: 0.6720

Epoch 11/20
63/63 [==============================] - 6s 83ms/step - loss: 0.5570 - accuracy: 0.7100 - val_loss: 0.5794 - val_accuracy: 0.6894

Epoch 12/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5708 - accuracy: 0.6970 - val_loss: 0.5655 - val_accuracy: 0.6968

Epoch 13/20
63/63 [==============================] - 6s 83ms/step - loss: 0.5566 - accuracy: 0.7185 - val_loss: 0.6122 - val_accuracy: 0.6671

Epoch 14/20
63/63 [==============================] - 6s 83ms/step - loss: 0.5564 - accuracy: 0.7190 - val_loss: 0.5510 - val_accuracy: 0.7104

Epoch 15/20
63/63 [==============================] - 5s 82ms/step - loss: 0.5559 - accuracy: 0.7185 - val_loss: 0.6130 - val_accuracy: 0.6522

Epoch 16/20
63/63 [==============================] - 6s 84ms/step - loss: 0.5542 - accuracy: 0.7145 - val_loss: 0.5925 - val_accuracy: 0.6931

Epoch 17/20
63/63 [==============================] - 6s 85ms/step - loss: 0.5481 - accuracy: 0.7140 - val_loss: 0.5809 - val_accuracy: 0.6993

Epoch 18/20
63/63 [==============================] - 6s 82ms/step - loss: 0.5461 - accuracy: 0.7135 - val_loss: 0.5554 - val_accuracy: 0.7351

Epoch 19/20
63/63 [==============================] - 6s 83ms/step - loss: 0.5277 - accuracy: 0.7345 - val_loss: 0.5554 - val_accuracy: 0.7364

Epoch 20/20
63/63 [==============================] - 6s 83ms/step - loss: 0.5287 - accuracy: 0.7270 - val_loss: 0.5311 - val_accuracy: 0.7426

<matplotlib.legend.Legend at 0x7f9fb3ebe290>

![plot-5-6.png]({{ site.baseurl }}/assets/images/plot-5-6.png)

**The accuracy of my model stabilized between 65% and 70% during training.**

My validation accuracy for `model3` is 10% better than `model1`.

In the visualization, training accuracy and validation accuracy intertwine across epochs. This indicates no overfitting occurred.

## Transfer Learning

We finally use transfer learning, which uses a pre-existing base model and incorporates it into a full model. We use the `MobileNetV2` model as a pre-existing base model in this case.

```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])

# usage of preprocessor, RandomFlip(), RandomRotation(0.2), base_model_layer, GlobalMaxPooling2D(), Dropout(0.2), and Dense(2)
model4 = tf.keras.Sequential([
    preprocessor,
    layers.RandomFlip(),
    layers.RandomRotation(0.2),
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.2),
    layers.Dense(2)
])

model4.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ["accuracy"])

history = model4.fit(train_dataset, 
                     epochs = 20, 
                     validation_data = validation_dataset)

plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

Epoch 1/20
63/63 [==============================] - 11s 111ms/step - loss: 1.1284 - accuracy: 0.7300 - val_loss: 0.1706 - val_accuracy: 0.9480

Epoch 2/20
63/63 [==============================] - 6s 95ms/step - loss: 0.5941 - accuracy: 0.8540 - val_loss: 0.1338 - val_accuracy: 0.9666

Epoch 3/20
63/63 [==============================] - 6s 94ms/step - loss: 0.4909 - accuracy: 0.8795 - val_loss: 0.1336 - val_accuracy: 0.9653

Epoch 4/20
63/63 [==============================] - 6s 95ms/step - loss: 0.4767 - accuracy: 0.8855 - val_loss: 0.1161 - val_accuracy: 0.9666

Epoch 5/20
63/63 [==============================] - 6s 94ms/step - loss: 0.4394 - accuracy: 0.8930 - val_loss: 0.1183 - val_accuracy: 0.9703

Epoch 6/20
63/63 [==============================] - 6s 93ms/step - loss: 0.4073 - accuracy: 0.8975 - val_loss: 0.1707 - val_accuracy: 0.9542

Epoch 7/20
63/63 [==============================] - 6s 92ms/step - loss: 0.3740 - accuracy: 0.9085 - val_loss: 0.1084 - val_accuracy: 0.9691

Epoch 8/20
63/63 [==============================] - 6s 93ms/step - loss: 0.3490 - accuracy: 0.9045 - val_loss: 0.1031 - val_accuracy: 0.9678

Epoch 9/20
63/63 [==============================] - 6s 94ms/step - loss: 0.3346 - accuracy: 0.9025 - val_loss: 0.0787 - val_accuracy: 0.9740

Epoch 10/20
63/63 [==============================] - 6s 93ms/step - loss: 0.3497 - accuracy: 0.9060 - val_loss: 0.1075 - val_accuracy: 0.9691

Epoch 11/20
63/63 [==============================] - 6s 92ms/step - loss: 0.3383 - accuracy: 0.9015 - val_loss: 0.1281 - val_accuracy: 0.9592

Epoch 12/20
63/63 [==============================] - 6s 94ms/step - loss: 0.3551 - accuracy: 0.8965 - val_loss: 0.1170 - val_accuracy: 0.9641

Epoch 13/20
63/63 [==============================] - 6s 93ms/step - loss: 0.2966 - accuracy: 0.9125 - val_loss: 0.0947 - val_accuracy: 0.9715

Epoch 14/20
63/63 [==============================] - 6s 93ms/step - loss: 0.3073 - accuracy: 0.9080 - val_loss: 0.1012 - val_accuracy: 0.9666

Epoch 15/20
63/63 [==============================] - 6s 94ms/step - loss: 0.2773 - accuracy: 0.9190 - val_loss: 0.0877 - val_accuracy: 0.9703

Epoch 16/20
63/63 [==============================] - 6s 94ms/step - loss: 0.4072 - accuracy: 0.8890 - val_loss: 0.0751 - val_accuracy: 0.9752

Epoch 17/20
63/63 [==============================] - 6s 93ms/step - loss: 0.3747 - accuracy: 0.9085 - val_loss: 0.0962 - val_accuracy: 0.9715

Epoch 18/20
63/63 [==============================] - 6s 92ms/step - loss: 0.3005 - accuracy: 0.9165 - val_loss: 0.0814 - val_accuracy: 0.9728

Epoch 19/20
63/63 [==============================] - 6s 93ms/step - loss: 0.2388 - accuracy: 0.9195 - val_loss: 0.0928 - val_accuracy: 0.9740

Epoch 20/20
63/63 [==============================] - 6s 93ms/step - loss: 0.2838 - accuracy: 0.9170 - val_loss: 0.0787 - val_accuracy: 0.9777

<matplotlib.legend.Legend at 0x7f9fb3c9fed0>

![plot-5-7.png]({{ site.baseurl }}/assets/images/plot-5-7.png)

**The accuracy of my model stabilized between 95% and 100% during training.**

My validation accuracy for `model4` is between 40% better than `model1`.

In the visualization, training accuracy is below validation accuracy intertwine across epochs. This indicates no overfitting occurred.

## Score on Test Data

We finally evaluate our final model on our `test_dataset`.

```python
model4.evaluate(test_dataset)
```

6/6 [==============================] - 1s 67ms/step - loss: 0.0773 - accuracy: 0.9740
[0.07727044075727463, 0.9739583134651184]

We see that accuracy is 97%—a great success! We can accurately classify cat and dog images now.